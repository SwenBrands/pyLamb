# -*- coding: utf-8 -*-

''''This script plots composites maps for the 27 LWTs for a given location and time period'''

#load packages
import numpy as np
np.seterr(divide='ignore', invalid='ignore')
import pandas as pd
import xarray as xr
from scipy import fft, arange, signal
import cartopy
import cartopy.crs as ccrs
from matplotlib import pyplot as plt
import os
from sklearn import preprocessing
from joblib import Parallel, delayed
import time
from statsmodels.tsa.arima_process import ArmaProcess
import statsmodels.api as sm
import statsmodels.tools.rng_qrng as rng_qrng #this was tried to reset random number generation
import random
from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER
exec(open('analysis_functions.py').read())

#set input parameter
n_par_jobs = 16
ensemble = 'cera20c' #cera20c or mpi_esm_1_2_hr
city = ['Barcelona'] #['Barcelona','Prague','Paris','Bergen'] #['Athens','Azores','Barcelona','Bergen','Cairo','Casablanca','Paris','Prague','SantiagoDC','Seattle','Tokio'] #city or point of interest
tarmonths = [1,2,3,4,5,6,7,8,9,10,11,12] #target months
taryears = [1901,2010] #start and end year
tarhours = [0,6,12,18]
tarwts = [1] #[5,13,22] direcciones sur, [9,17,26] direcciones norte
figs = '/lustre/gmeteo/WORK/swen/datos/tareas/lamb_cmip5/figs' #path to the output figures
store_wt_orig = '/lustre/gmeteo/WORK/swen/datos/tareas/lamb_cmip5/results_v2/' #path to input Lamb Weather types previously generated by 1) interpolator_xesmf.py and 2) makecalcs_parallel.py
store_sst_origfile = '/lustre/gmeteo/WORK/swen/datos/OBSdata/hadisst/HadISST_sst.nc' #complete path including filename of the SST input data used for cross-spectrum analysis with LWT counts.
sst_latlim = [0,70] #[40,50] [0,70] #latitude limits used to cut out sst data from South to North
sst_lonlim = [-95,45] #[-180,180] [-30, 0] [-180,180] #respective longitude limits, from West to East
region = 'nh' #only used for mapping purposes; defines the projection to be used
skipna = True #if set to TRUE, skips missing values when calculating temporal aggregation. If set to False assigns nan to the entire grid-box time series when one nan is present (check this !) 
meanperiod = 10 #running-mean period in years

#options for calculating the sst gradient index
sstind_latlim = [0,50] #box for calculation of sst gradient index
sstind_lonout1 = [-10,145] #these longitudes will be excluded from calculation of the index
sstind_lonout2 = [-130,-60] #if exclude1 and 2 are applied, only the North Atlantic is covered

#options used for periodgram, experimental so far, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html
fs = 1 #sampling frequency for 3-hourly data, 30*8 is monthly, 90*8 is seasonal, alternatively 1 for yearly data
nfft_quotient = None #integer setting the quotient or None for default options, this is the fraction of the time series used as lenght of the FFT (i.e. the number of frequencies/periods to be evaluated), 4 is recommended by Schönwiese 2006; if set to None, the default nfft option is used in signal.periodgram(). This option is not used in signal.csd() which always assumes None / default options 
window = 'hann' #http://qingkaikong.blogspot.com/2017/01/signal-processing-finding-periodic.html
scaling = 'spectrum'
repetitions = 100 #10000 is ideal
detrend = 'linear' #False, linear or constant for removing the linear trend or mean only prior to calculating the power spectrum
ci_percentiles = [2.5,97.5] #and this one will be plotted
cutoff = [] #defines where the x-axis is cut-off in the time-series plots, [] for no cutoff
yearly_units = '%' # count or % (relative frequency); unit of the yearly LWT counts
relax = 0
acorr_lag = 1 #the lag of the autocorrelation function used to generate the AR(acorrr_lag) process used to generate noise for obtainting critical values for randomly generated power spectra, 1 for red noise process

#visualization options
dpival = 300 #resolution of the output figure in dots per inch
outformat = 'png' #png, pdf, etc.
titlesize = 6. #Font size of the titles in the figures
colormap_cps = 'PiYG' #RdYlGn, PiYG, seismic

#auxiliary variables to be used in future versions of the script; currently not used
aggreg = 'year' #unlike map_lowfreq_var.py, this script currently only works for aggreg = 'year', other aggregation levels will be implemented in the future
anom = 'no' #not relevant here since yearly counts are considered, i.e. the annual cycle is not present in the time series, is kept for future versions of the script

#execute ###############################################################################################
starttime = time.time()
print('INFO: This script will use '+str(n_par_jobs)+' parallel jobs to calculate cross power spectra from randomly re-shuffled time series in order to obtain critical values for significance testing.')
#open netCDF file containing SSTs and filter out temporal and spatial subset
nc_sst = xr.open_dataset(store_sst_origfile)
nc_sst = nc_sst.drop('time_bnds') #drop unnecessary variables
#retain only postive values, values equal or below -1.8ºC are ice not water, -1.8º is the freezing point for salty water
nc_sst = nc_sst.where(nc_sst['sst'] > -1.8) #sets values <= -1.8ºC to nan
dates_sst = pd.DatetimeIndex(nc_sst.time.values) #note that the first four time stamps in HadSST input netCDF are in strange format, e.g. 1870-01-16 11:59:59.505615234
#get temporal subset
year_ind_sst = np.where((dates_sst.year >= taryears[0]) & (dates_sst.year <= taryears[1]))[0]
nc_sst = nc_sst.isel(time=year_ind_sst)
dates_sst = dates_sst[year_ind_sst]
#get spatial subset and get lons and lats from the reduced domain
lon_ind_sst = np.where((nc_sst['longitude'].values >= sst_lonlim[0]) & (nc_sst['longitude'].values <= sst_lonlim[1]))[0]
lat_ind_sst = np.where((nc_sst['latitude'].values >= sst_latlim[0]) & (nc_sst['latitude'].values <= sst_latlim[1]))[0]
nc_sst = nc_sst.isel(longitude = lon_ind_sst,latitude = lat_ind_sst)
lons = nc_sst.longitude.values
lats = nc_sst.latitude.values
#get annual mean values
if aggreg == 'year':
     nc_sst = nc_sst.groupby('time.year').mean(dim='time',skipna=skipna)
else:
    raise Exception("ERROR: unknown entry for <aggreg> input parameter. Temporal aggregations others than aggreg = 'year' are currently not supported.")

#calculate SST gradient index from previously calculated SST subset array
nc_sstind = nc_sst.copy()
lon_ind_sstind = np.where((nc_sstind['longitude'].values <= sstind_lonout1[0]) | (nc_sstind['longitude'].values >= sstind_lonout1[1]))[0]
nc_sstind = nc_sstind.isel(longitude=lon_ind_sstind)
lon_ind_sstind = np.where((nc_sstind['longitude'].values <= sstind_lonout2[0]) | (nc_sstind['longitude'].values >= sstind_lonout2[1]))[0]
nc_sstind = nc_sstind.isel(longitude=lon_ind_sstind)
#gr_time_lon = np.nanmean(np.gradient(nc_sstind['sst'].values,axis=1),axis=1) #returns the mean meridional gradient per longitude bin, numpy array
gr_time_lon = np.nanmax(np.gradient(nc_sstind['sst'].values,axis=1),axis=1) #returns the max meridional gradient per longitude bin, numpy array
gr_time = np.nanmean(gr_time_lon,axis=1) #returns an average of the former along all longitudes, numpy array, this is the final, non-standardized meridional SST gradient index averaged over domain defined by the user
# gr_time = nc_sstind.sst.diff(dim='latitude').mean(dim='latitude').mean(dim='longitude').values #alternatively calculated the mean meridional difference instead of the gradient

# #standardize (or z-transform) SST data along the time axis and set nan values to 0 and retain a numpy array <z_sst>
z_sst = z_transform(nc_sst['sst'].values)
nangrid = np.any(np.isnan(z_sst),axis=0) # lat x lon grid returning True if one or more timesteps in z_sst are nan at a given grid-box
z_sst[np.isnan(z_sst)] = 0. #set nans to 0 for processing with signal.csd() below

if ensemble == 'cera20c':
    model = ['cera20c','cera20c','cera20c','cera20c','cera20c','cera20c','cera20c','cera20c','cera20c','cera20c']
    mrun = ['m0','m1','m2','m3','m4','m5','m6','m7','m8','m9']
    experiment = '20c'
    model_label = 'CERA-20C'
elif ensemble == 'mpi_esm_1_2_hr':
    model = ['mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr','mpi_esm_1_2_hr']
    mrun = ['r1i1p1f1','r2i1p1f1','r3i1p1f1','r4i1p1f1','r5i1p1f1','r6i1p1f1','r7i1p1f1','r8i1p1f1','r9i1p1f1','r10i1p1f1']
    experiment = 'historical'
    model_label = 'MPI-ESM1.2-HR'
else:
    raise Exception('ERROR: unknown entry for <ensemble> input parameter !') 

print('INFO: get low frequency variability for '+aggreg+' LWT counts, '+ensemble+' with '+str(len(mrun))+' runs, '+str(tarmonths)+' months, '+str(taryears)+' years, '+str(tarhours)+' hours, ' +window+' window, '+detrend+' detrending and '+str(repetitions)+' repetitions for the resampling approach. Output units are: '+yearly_units+'.')

seaslabel = str(tarmonths).replace('[','').replace(']','').replace(', ','')
if seaslabel == '123456789101112':
    seaslabel = 'yearly'
    
wtnames = ['PA', 'DANE', 'DAE', 'DASE', 'DAS', 'DASW', 'DAW', 'DANW', 'DAN', 'PDNE', 'PDE', 'PDSE', 'PDS', 'PDSW', 'PDW', 'PDNW', 'PDN', 'PC', 'DCNE', 'DCE', 'DCSE', 'DCS', 'DCSW', 'DCW', 'DCNW', 'DCN', 'U']
wtlabel = str(np.array(wtnames)[np.array(tarwts)-1]).replace("[","").replace("]","").replace("'","")

wt_agg = np.zeros((len(model),len(list(range(taryears[0],taryears[1]+1)))))
for cc in np.arange(len(city)):
    print('INFO: obtaining results for '+city[cc]+'....')
    #get target location
    tarlat,tarlon = get_location(city[cc])
    if tarlon < 0:
        tarlon = 360+tarlon # convert target longitude into 0 - 360 degrees format
    #detect the hemisphere
    if tarlat > 0:
        hemis = 'nh'
    elif tarlat <= 0:
        hemis = 'sh'
    else:
        raise Exception('ERROR: unknown entry for <tarlat>!')

    for mm in list(range(len(model))):
        if model[mm] == 'cera20c':
            timestep = '3h'
            file_startyear = '1901'
            file_endyear = '2010'
        elif model[mm] == 'era5':
             timestep = '6h'
             file_startyear = '1979'
             file_endyear = '2020'
        elif model[mm] in ('mpi_esm_1_2_hr','ec_earth3_veg'):
             timestep = '6h'
             file_startyear = '1850'
             file_endyear = '2014'
        else:
            timestep = '6h'
            file_startyear = '1979'
            file_endyear = '2005'
            
        store_wt = store_wt_orig+'/'+timestep+'/'+experiment+'/'+hemis
        wt_file = store_wt+'/wtseries_'+model[mm]+'_'+experiment+'_'+mrun[mm]+'_'+hemis+'_'+str(file_startyear)+'_'+str(file_endyear)+'.nc' #path to the LWT catalogues

        #load the LWT time series for the centerlat and target years obtained above
        wt = xr.open_dataset(wt_file)
        
        #get the gridbox nearest to tarlon and tarlat
        lon_center = wt.lon.sel(lon=tarlon,method='nearest').values
        lat_center = wt.lat.sel(lat=tarlat,method='nearest').values
        #lon_center = np.array(lon_center-180.)
        
        wt_center = wt.sel(lon=lon_center,lat=lat_center,method='nearest')
        dates_wt = pd.DatetimeIndex(wt_center.time.values)
        #select requested time period (years and hours)
        year_ind_wt = np.where((dates_wt.year >= taryears[0]) & (dates_wt.year <= taryears[1]) & (np.isin(dates_wt.hour,tarhours)))[0]
        wt_center = wt_center.isel(time=year_ind_wt)
        #wt_val = wt_center.wtseries.values

        bin_array = np.zeros(wt_center.wtseries.shape)
        tarwt_ind = np.where(wt_center.wtseries.isin(tarwts))
        bin_array[tarwt_ind] = 1
        arr_tarwts = xr.DataArray(data=bin_array,coords=[pd.DatetimeIndex(wt_center.time)],dims='time',name='wtseries')
        
        #get time series with yearly target WT counts
        if aggreg == 'year':
            if yearly_units == 'count':
                wt_agg_step = arr_tarwts.groupby('time.year').sum(dim='time')
            elif yearly_units == '%':
                hours_per_year = arr_tarwts.groupby('time.year').count()
                wt_agg_step = arr_tarwts.groupby('time.year').sum('time') #calculate annual mean values
                wt_agg_step = wt_agg_step / hours_per_year *100
            else:
                raise Exception('ERROR: unknown entry for <yearly_units>!')       
        
        wt_agg[mm,:] = z_transform(wt_agg_step.values)
        ntime = wt_agg.shape[1]
        
        #experimental part of the script, get the frequency and spectrum for yearly ensemble mean WT counts
        if nfft_quotient == None:
            print('Info: The default nfft options in signal.periodogram() will be used for spectral analysis...')
            nfft = None
        elif nfft_quotient > 0:
            nfft = int(np.floor(ntime/nfft_quotient)) #use optionally in the next 4 lines
            print('Info: As requested by the user, '+str(nfft)+' ffts will be used for spectral analysis...')
        else:
            raise Exception('ERROR: unknown entry for <nfft_quotient>!')
        
        #get empirical cross spectrum amplitudes
        wt_agg_repeat = np.tile(np.expand_dims(np.expand_dims(wt_agg[mm,:],axis=1),axis=1),(1,z_sst.shape[1],z_sst.shape[2]))
        fq, CPxx_step = signal.csd(wt_agg_repeat, z_sst, fs=fs, window=window, noverlap=None, nperseg=nfft, detrend=detrend, return_onesided=True, scaling='spectrum', axis=0, average='mean')
        
        #initialize output arrays of spectral analysis       
        if mm == 0:
            CPxx = np.zeros((len(model),CPxx_step.shape[0],CPxx_step.shape[1],CPxx_step.shape[2]))
            CPxx_ci = np.zeros((len(model),len(ci_percentiles),CPxx_step.shape[0],CPxx_step.shape[1],CPxx_step.shape[2]))

        #get confidence intervals from cross spectrum amplitudes obtained from random time series following an AR(1) autoregressive process / red noise fitted to the LWT time series
        #init arrays which to be filled with random amplitudes
        CPxx_rand = np.zeros((repetitions,CPxx_step.shape[0],CPxx_step.shape[1],CPxx_step.shape[2]))
        CPxx_rand_par = np.zeros((repetitions,CPxx_step.shape[0],CPxx_step.shape[1],CPxx_step.shape[2]))
        
        #parallel part to generate red noise
        print('INFO: Initializing the parallel part of the script....')
        par_result = Parallel(n_jobs=n_par_jobs)(delayed(get_csd_ar)(wt_agg[mm,:], z_sst, nangrid, ntime, acorr_lag, fs, window, nfft, detrend) for rr in np.arange(repetitions))
        for rr in np.arange(len(par_result)):
            CPxx_rand_par[rr,:,:,:] = par_result[rr]
        #del par_result
        ##paralell part ends here
        
        #sequential way to generate red noise
        #get AR1 process with lag-1 autocorrelation coefficient from wt_agg (containing the LWT time series relevant here), see https://goodboychan.github.io/python/datacamp/time_series_analysis/2020/06/08/01-Autoregressive-Models.html
        acorr_wt = sm.tsa.acf(wt_agg[mm,:], nlags = ntime)
        arpar_wt_1 = np.array([1, acorr_wt[acorr_lag]*-1]) #ArmaProcess() function called below requires to switch the sign of the autocorrelation coefficient 
        arpar_wt_2 = np.array([1])
        AR_object1_wt = ArmaProcess(arpar_wt_1, arpar_wt_2)
        
        for rr in np.arange(repetitions):
            #get random AR(arcorr_lag) time series and expand to fit dims of z_sst 
            rand_wt = AR_object1_wt.generate_sample(nsample=ntime)
            rand_wt = np.tile(np.expand_dims(np.expand_dims(rand_wt,axis=-1),axis=-1),[1,z_sst.shape[1],z_sst.shape[2]])
            
            rand_sst = np.zeros(z_sst.shape)
            for ii in np.arange(z_sst.shape[1]):
                for jj in np.arange(z_sst.shape[2]):
                    if nangrid[ii,jj] == True:
                        #print('Info: nans are present at '+str(ii)+' and '+str(jj)+'. Skipping...')
                        continue
                    acorr_sst = sm.tsa.acf(z_sst[:,ii,jj], nlags = ntime)
                    arpar_sst_1 = np.array([1, acorr_sst[acorr_lag]*-1]) #ArmaProcess() function called below requires to switch the sign of the autocorrelation coefficient 
                    arpar_sst_2 = np.array([1])
                    AR_object1_sst = ArmaProcess(arpar_sst_1, arpar_sst_2)
                    rand_sst_step = AR_object1_sst.generate_sample(nsample=ntime)
                    if np.any(np.isnan(rand_sst_step)): #rand_sst_step may contain nans that cannot be processed by signal.csd(), so skip in nan is found a one or mor timesteps
                        continue
                    rand_sst[:,ii,jj] = rand_sst_step
                
            fq_rand_step, CPxx_rand[rr,:,:,:] = signal.csd(rand_wt, rand_sst, fs=fs, window=window, nperseg=nfft, noverlap=None, detrend=detrend, return_onesided=True, scaling='spectrum', axis=0, average='mean')
        #sequential part ends here
        
        #get 95% confidence intervals for the random power spectra and fill the pre-initialized arrays
        CPxx_ci[mm,:,:,:,:] = np.percentile(CPxx_rand,ci_percentiles,axis=0)
        CPxx[mm,:,:,:] = CPxx_step
        #set nans from nangrid defined above
        nanbool = np.tile(np.expand_dims(np.expand_dims(nangrid,0),0),[CPxx_rand.shape[0],CPxx_rand.shape[1],1,1])
        CPxx_rand[nanbool] = np.nan
        CPxx_rand_par[nanbool] = np.nan        
        wt.close() #close LWT file

    ##get mask indicating grid-boxes where the cross power spectra for a given period are significant in len(model)-relax ensemble members, critical values above which significance is assumed were obtained above from random reshuffles of the original time series
    period = 1/fq
    period[np.isinf(period)] = np.nan
    signif_cps = np.copy(CPxx)
    sigmask_cps = (CPxx < CPxx_ci[:,0,:,:,:]) | (CPxx > CPxx_ci[:,1,:,:,:])
    spurmask_cps = (CPxx >= CPxx_ci[:,0,:,:,:]) & (CPxx <= CPxx_ci[:,1,:,:,:])
    signif_cps[sigmask_cps] = 1
    signif_cps[spurmask_cps] = 0
    #signif_cps[spurmask_cps == False] = 1
    #signif_cps[spurmask_cps == True] = 0
    sumsignif_cps = np.sum(signif_cps,axis=0)
    
    sign_sum = np.copy(CPxx)
    posmask = sign_sum > 0
    negmask = sign_sum <= 0
    sign_sum[posmask] = 1
    sign_sum[negmask] = -1
    sign_sum = np.sum(sign_sum,axis=0)
    #agree_ind_tr = (np.abs(slope_sum) >= (len(model)-relax)) & (sumsignif_tr >= (len(model)-relax))
    agree_ind_cps = (np.abs(sign_sum) >= (len(model)-relax)) & (sumsignif_cps >= len(model)-relax)
    disagree_ind_cps = (np.abs(sign_sum) < (len(model)-relax)) | (sumsignif_cps < len(model)-relax) #disagreement index, currently not in use
    
    #plot power spectrum agreement maps for each period
    print('INFO: Power spectra have been calcualted for the following '+str(len(period))+' periods:')
    print(period)
    CPxx_med = np.median(CPxx,axis=0)
    maxval_cps = np.max(np.abs(CPxx_med))
    #minval_cps = 0
    minval_cps = maxval_cps*-1.
    
    #set geographical variables used in both kind of maps (trends and power spectra)
    xx,yy = np.meshgrid(lons,lats)
    xx = np.transpose(xx)
    yy = np.transpose(yy)
    runlabel = str(len(mrun)) #number of runs
    modellabel = model[mm] #last model
    halfres = np.abs(np.diff(lats))[0]/2
            
    if region == 'nh':
        map_proj = ccrs.NorthPolarStereo() #ccrs.PlateCarree()
        #map_proj = ccrs.PlateCarree() #ccrs.PlateCarree()
    elif region == 'sh':
        map_proj = ccrs.SouthPolarStereo()
    else:
        raise Exception('ERROR: check entry for <region>!')
            
    #make output directory if it does not exist
    wtlabel2save = wtlabel.replace(' ','_')
    path_periodogram = figs+'/'+ensemble+'/local/'+city[cc]+'/'+aggreg+'/maps/'+region+'/'+seaslabel+'/csd_periodogram/'+wtlabel2save
    if os.path.isdir(path_periodogram) != True:
        os.makedirs(path_periodogram)
    
    for pp in np.arange(len(period)):
        if agree_ind_cps[pp,:,:].flatten().sum()==0:
            print('INFO: No ensemble agreement as defined by the user (relax: '+str(relax)+') was found for the '+str(np.round(period[pp],1))+' years period. Proceed to the next period....')
            continue
        
        title_cps = 'CPS '+city[cc]+' '+wtlabel+'-SST '+str(np.round(period[pp],1))+'y '+window+' nfft'+str(nfft)+' ci'+str(round(np.diff(ci_percentiles)[0]))+' '+modellabel.upper()+' '+runlabel+'m relax'+str(relax)+' '+str(taryears[0])+' '+str(taryears[1])+'-'+seaslabel+' '+aggreg+' dtr '+detrend+' an '+anom+' '+region
        cbarlabel = 'Ens median cross spectrum amplitude (z-score$²$)'
        savename_cps = path_periodogram+'/'+city[cc]+'_cps_sst_'+str(np.round(period[pp],1))+'y_'+window+'_nfft'+str(nfft)+'_ci'+str(round(np.diff(ci_percentiles)[0]))+'_'+modellabel+'_'+runlabel+'m_relax'+str(relax)+'_'+wtlabel2save+'_'+str(taryears[0])+'_'+str(taryears[1])+'_'+seaslabel+'_'+aggreg+'_dtr_'+detrend+'_an_'+anom+'_'+region+'.'+outformat
        get_map_lowfreq_var(np.transpose(CPxx_med[pp,:,:]),xx,yy,np.transpose(agree_ind_cps[pp,:,:]),minval_cps,maxval_cps,dpival,title_cps,savename_cps,halfres,colormap_cps,titlesize,cbarlabel,origpoint=[tarlon, tarlat])

#close SST files and measure elapsed time in seconds
nc_sst.close()
nc_sstind.close()
endtime = time.time()
elaptime = endtime - starttime
print('INFO: get_csd_local_matrix.py has ended successfully! The elapsed time is '+str(elaptime)+'seconds, exiting now...')
quit()
