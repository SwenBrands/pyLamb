# -*- coding: utf-8 -*-

''''This script calculates and plots Lamb Weather Type time-series and signal-to-noise ratios at specified locations'''

#load packages
import numpy as np
import pandas as pd
import xarray as xr
from scipy import fft, arange, signal
import cartopy
import cartopy.crs as ccrs
from matplotlib import pyplot as plt
import os
from statsmodels.tsa.arima_process import ArmaProcess
import statsmodels.api as sm
import xskillscore as xs
import pdb as pdb #then type <pdb.set_trace()> at a given line in the code below
exec(open('analysis_functions.py').read())
exec(open('get_historical_metadata.py').read()) #a function assigning metadata to the models in <model> (see below)

#set input parameter
obs = 'era5' #cera20c or mpi_esm_1_2_hr or ec_earth3
ensemble = ['ec_earth3','ec_earth3','ec_earth3','ec_earth3'] #cera20c or mpi_esm_1_2_hr or ec_earth3
experiment = ['dcppA','dcppA','dcppA','historical'] #historical, amip, piControl, 20c or dcppA, used to load the data
experiment_out = ['dcppA_1','dcppA_5','dcppA_10','historical'] #used as label in the xarray data array produced here; combines experiment with lead time if indicated
lead_time = [1,5,10,None] #lead time or forecast year or the dcppA LWT data
start_years = [1961,1965,1970,1961] #list containing the start years of the experiment defined in <experiment>
end_years = [2019,2023,2028,2028] #list containing the end years of the experiment defined in <experiment>

ensemble_color = ['orange','black','grey','blue']
ensemble_linestyle = ['dashed','dashed','solid','dotted']

reference_period = [1970,2014] # "from_data" or list containing the start and end years
seasons = ['JJA','DJF'] #list of seasons to be considered: year, DJF, MAM, JJA or SON
#tarwts = [[1],[2,10,19],[3,11,20],[4,12,21],[5,13,22],[6,14,23],[7,15,24],[8,16,25],[9,17,26],[18],[27]] #list of lists containing target LWTs for which monthly frequencies will be stored
tarwts = [['PA'], ['DANE','PDNE','DCNE'], ['DAE','PDE','DCE'], ['DASE','PDSE','DCSE'], ['DAS','PDS','DCS'], ['DASW','PDSW','DCSW'], ['DAW','PDW','DCW'], ['DANW','PDNW','DCNW'], ['DAN','PDN','DCN'], ['PC'], ['U']] #original names for 11 types
tarwts_name = ['PA','NE','E','SE','S','SW','W','NW','N','PC','U'] #summarized names for 11 types
# tarwts_name = ['PA', 'DANE_PDNE_DCNE', 'DAE_PDE_DCE', 'DASE_PDSE_DCSE', 'DAS_PDS_DCS', 'DASW_PDSW_DCSW', 'DAW_PDW_DCW', 'DANW_PDNW_DCNW', 'DAN_PDN_DCN','PC','U'] #original names for 11 types

center_wrt = 'memberwise_mean' # ensemble_mean or memberwise_mean; centering w.r.t. to ensemble (or overall) mean value or member-wise temporal mean value prior to calculating signal-to-noise
figs = '/lustre/gmeteo/WORK/swen/datos/tareas/lamb_cmip5/figs/ec_earth3' #base path to the output figures
store_wt_orig = '/lustre/gmeteo/WORK/swen/datos/tareas/lamb_cmip5/results_v2'
meanperiod = 10 #running-mean period in years
std_critval = 1.28 #1 = 68%, 1.28 = 80 %, 2 = 95%; standard deviation used to define the critical value above or below which the signal-to-noise ratio is assumed to be significant.
rho_ref = '20c_era5' #20c_era5 or 20c_cera20c; reference observational dataset used to calculate correlations. Must be included in the <experiment_out> input parameter defined above

#options used for periodgram, experimental so far, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.periodogram.html
yearly_units = '%' # count, % (relative frequency) or z-score; unit of the yearly LWT counts

##visualization options
map_proj_nh = ccrs.NorthPolarStereo() #ccrs.PlateCarree()
map_proj_sh = ccrs.SouthPolarStereo() #ccrs.PlateCarree()
axes_config = 'equal' # equal or individual; if set to equal, the axes limits are equal for all considered lead-times in a given city; if set to individual, the axes are optimized for each specific lead-time in that city
plot_sig_stn_only = 'no' #plot only significant signal-to-noise ratios in pcolor format, yes or no
dpival = 300 #resolution of the output figure in dots per inch
outformat = 'pdf' #png, pdf, etc.
titlesize = 8. #Font size of the titles in the figures
colormap = 'bwr'
edgecolor = 'black' #if set to any colour, e.g. black, cell boundaries are depicted in the pcolor plots generated by this script.

#auxiliary variables to be used in future versions of the script; currently not used
aggreg = 'year' #unlike map_lowfreq_var.py, this script currently only works for aggreg = 'year', other aggregation levels will be implemented in the future
anom = 'no' #not relevant here since yearly counts are considered, i.e. the annual cycle is not present in the time series, is kept for future versions of the script

##execute ###############################################################################################
if aggreg != 'year': #check for correct usage of the script
    raise Exception("Bad call of the script: This script currently only works for yearly LWT counts, i.e. aggreg = 'year' !)")

timestep = 'mon'
taryears = [np.max(start_years),np.min(end_years)] #years common to all experiments defined in <experiment>

ref_period = reference_period
print('The reference period used for anomaly calculation is '+str(ref_period))

# wtnames = ['PA', 'DANE', 'DAE', 'DASE', 'DAS', 'DASW', 'DAW', 'DANW', 'DAN', 'PDNE', 'PDE', 'PDSE', 'PDS', 'PDSW', 'PDW', 'PDNW', 'PDN', 'PC', 'DCNE', 'DCE', 'DCSE', 'DCS', 'DCSW', 'DCW', 'DCNW', 'DCN', 'U']
# wtlabel = str(np.array(wtnames)[np.array(tarwts)-1]).replace("[","").replace("]","").replace("'","")

study_years = np.arange(np.array(taryears).min(),np.array(taryears).max()+1,1) #numpy array of years limited by the lower and upper end of considered years
t_startindex = int(meanperiod/2) #start index of the period to be plotted along the x axis
t_endindex = int(len(study_years)-(meanperiod/2-1)) #end index of the period to be plotted along the x axis

#init output arrays of the ensemble x city loop series
#for 3d arrays ensemble x city x study_years

# stn_all = np.zeros((len(seasons),len(lead_time),len(ensemble),len(city),len(study_years)))
# signal_all = np.copy(stn_all)
# noise_all = np.copy(stn_all)
# runmeans_all = np.copy(stn_all)

#CALCULATIONS part of the script
##loop through all seasons, sea
for lwt in np.arange(len(tarwts)):
    for sea in np.arange(len(seasons)):
        seaslabel = seasons[sea]
        ##loop throught the lead-times, lt
        for en in np.arange(len(experiment)):
            print('Aggregating '+str(tarwts[lwt])+' type(s) labelled as '+tarwts_name[lwt]+' for season '+seasons[sea]+' and experiment '+experiment[en]+' with lead-time '+str(lead_time[en])+' over the NH and SH...')
            #get ensemble configuration as defined in analysis_functions,py, see get_ensemble_config() function therein
            model,mrun,model_label,tarhours = get_ensemble_config(ensemble[en],experiment[en])
            
            print('INFO: get monthly LWT counts for '+aggreg+', '+ensemble[en]+' with '+str(len(mrun))+' runs, lead time '+str(lead_time[en])+', season '+str(seasons[sea])+', start year '+str(start_years[en])+' and end year '+str(end_years[en]))
           
            for mm in list(range(len(model))):
                #get metadata for this GCM, incluing start and end year label of the LWT input files to be loaded below
                if experiment[en] in ('amip','dcppA','historical','piControl'):
                    runspec,complexity,family,cmip,rgb,marker,latres_atm,lonres_atm,lev_atm,latres_oc,lonres_oc,lev_oc,ecs,tcr = get_historical_metadata(model[mm]) #check whether historical GCM configurations agree with those used in DCPPA ! 
                    file_taryears, timestep_h = get_target_period(model[mm],experiment[en],cmip_f=cmip,lead_time_f=lead_time[en]) #timestep_h is the temporal resolution of the instantaneous LWT file underlying the monthly counts obtained with makecalcs_parallel_plus_counts.py
                elif experiment[en] == '20c':
                    print('get_historical_metadata.py is not called for experiment = '+experiment[en]+'...')
                    file_taryears, timestep_h = get_target_period(model[mm],experiment[en])
                else:
                    raise Exception('ERROR: unknown entry in <experiment> input parameter')
                
                file_startyear = file_taryears[0]
                file_endyear = file_taryears[1]
                
                store_wt_nh = store_wt_orig+'/'+timestep+'/'+experiment[en]+'/nh'
                store_wt_sh = store_wt_orig+'/'+timestep+'/'+experiment[en]+'/sh'
                if experiment[en] == 'dcppA':
                    wt_file_nh = store_wt_nh+'/wtcount_mon_'+model[mm]+'_'+experiment[en]+'_'+mrun[mm]+'_nh_'+str(lead_time[en])+'y_'+str(file_startyear)+'_'+str(file_endyear)+'.nc' #path to the NH LWT catalogues
                    wt_file_sh = store_wt_sh+'/wtcount_mon_'+model[mm]+'_'+experiment[en]+'_'+mrun[mm]+'_sh_'+str(lead_time[en])+'y_'+str(file_startyear)+'_'+str(file_endyear)+'.nc' #path to the SH LWT catalogues
                elif experiment[en] == 'historical':
                    wt_file_nh = store_wt_nh+'/wtcount_mon_'+model[mm]+'_'+experiment[en]+'_'+mrun[mm]+'_nh_'+str(file_startyear)+'_'+str(file_endyear)+'.nc' #path to the NH LWT catalogues
                    wt_file_sh = store_wt_sh+'/wtcount_mon_'+model[mm]+'_'+experiment[en]+'_'+mrun[mm]+'_sh_'+str(file_startyear)+'_'+str(file_endyear)+'.nc' #path to the SH LWT catalogues
                else:
                    raise Exception('ERROR: check entry for experiment[en] !')
                #load the LWT time series for the centerlat and target years obtained above, merge them and close hemispheric files
                wt_nh = xr.open_dataset(wt_file_nh)
                wt_sh = xr.open_dataset(wt_file_sh)
                wt = xr.merge([wt_nh,wt_sh])
                wt_nh.close()
                wt_sh.close()
                del(wt_nh,wt_sh)
                
                #select requested season
                if seasons[sea] in ('DJF','MAM','JJA','SON'):
                    wt = wt.sel(time=(wt['time'].dt.season == seasons[sea]))
                    print('Processing '+seasons[sea]+' season...')
                elif seasons[sea] == 'year':
                    print('For season[sea] = '+seasons[sea]+', the entire calendar year will be considered.')
                else:
                    raise Exception('ERROR: Unknown entry for '+seasons[sea]+' !!')
                    
                # #add one year for DJF values
                # if seasons[sea] == 'DJF':
                    # print('Adding one year to December datetime64 index values for '+seasons[sea]+'...')
                    # dates_pd = pd.DatetimeIndex(wt.time.values)
                    # dates_pd[dates_pd.month==12][:] = dates_pd[dates_pd.month==12]+pd.DateOffset(years=1)
                    # wt.time.values[:] = dates_pd
                
                #select requested years
                dates_wt = pd.DatetimeIndex(wt.time.values)
                #year_ind_wt = np.where((dates_wt.year >= taryears[0]) & (dates_wt.year <= taryears[1]))[0]
                year_bool = (dates_wt.year >= taryears[0]) & (dates_wt.year <= taryears[1])
                wt = wt.isel(time=year_bool)
                
                #retain requested LWTs, sum along lwt dimension to get the joint counts per month
                wt = wt.sel(lwt=tarwts[lwt]).sum(dim='lwt')
                
                #calculate year-to-year relative frequencies (%)
                wt = wt.groupby('time.year').sum('time')
                wt.counts[:] = wt.counts / (wt.days_per_month*int(timestep_h[0]))*100
                
                #init numpy array to be filled with year-to-year relative LWT frequencies, loaded experiments and lead times
                if sea == 0 and en == 0 and mm == 0:
                    taryears_index = np.arange(taryears[0],taryears[1]+1) 
                    wt_rf_np = np.zeros((len(seasons),len(experiment),len(model),len(taryears_index),len(wt.lon),len(wt.lat)))
                    experiment_loaded = []
                    lead_time_loaded = []
                    #dimensions needed to convert wt_rf_np into xarray data array format
                    years = wt.year
                    lon = wt.lon
                    lat = wt.lat
                
                #assign
                wt_rf_np[sea,en,mm,:,:,:] = wt.counts.values
                
                #clean up
                wt.close()
                del(wt)
            
    #convert to xarray data array
    run_index = np.arange(len(mrun))
    wt_mod = xr.DataArray(wt_rf_np, coords=[seasons, experiment_out, run_index, years, lon, lat ], dims=['season','experiment','run_index','time','lon','lat'], name='counts')
    wt_mod.attrs['units'] = 'relative frequency per year (%)'

    #load observations
    for sea in np.arange(len(seasons)):
        store_wt_nh_obs = store_wt_orig+'/'+timestep+'/historical/nh'
        store_wt_sh_obs = store_wt_orig+'/'+timestep+'/historical/sh'
        wt_file_nh_obs = store_wt_nh_obs+'/wtcount_mon_era5_historical_r1i1p1_nh_1940_2022.nc' #path to the NH LWT catalogues
        wt_file_sh_obs = store_wt_sh_obs+'/wtcount_mon_era5_historical_r1i1p1_sh_1940_2022.nc' #path to the SH LWT catalogues

        wt_nh_obs = xr.open_dataset(wt_file_nh_obs)
        wt_sh_obs = xr.open_dataset(wt_file_sh_obs)
        wt_obs = xr.merge([wt_nh_obs,wt_sh_obs])
        wt_nh_obs.close()
        wt_sh_obs.close()
        del(wt_nh_obs,wt_sh_obs)

        #select requested season as above for model data
        if seasons[sea] in ('DJF','MAM','JJA','SON'):
            wt_obs = wt_obs.sel(time=(wt_obs['time'].dt.season == seasons[sea]))
            print('Processing '+seasons[sea]+' season...')

        #select requested years
        dates_wt_obs = pd.DatetimeIndex(wt_obs.time.values)
        year_bool_obs = (dates_wt_obs.year >= taryears[0]) & (dates_wt_obs.year <= taryears[1])
        wt_obs = wt_obs.isel(time=year_bool_obs)

        #retain requested LWTs, sum along lwt dimension to get the joint counts per month
        wt_obs = wt_obs.sel(lwt=tarwts[lwt]).sum(dim='lwt')

        #calculate year-to-year relative frequencies (%)
        wt_obs = wt_obs.groupby('time.year').sum('time')
        wt_obs.counts[:] = wt_obs.counts / (wt_obs.days_per_month*int(timestep_h[0]))*100
        
        #init numpy array to be filled with year-to-year relative LWT frequencies, loaded experiments and lead times
        if sea == 0:
            wt_rf_np_obs = np.zeros((len(seasons),len(taryears_index),len(wt_obs.lon),len(wt_obs.lat)))
            #dimensions needed to convert wt_rf_np into xarray data array format
            years = wt_obs.year
            lon = wt_obs.lon
            lat = wt_obs.lat
        
        #assign
        wt_rf_np_obs[sea,:,:,:] = wt_obs.counts.values
        
        #clean up
        wt_obs.close()
        del(wt_obs)

    #convert to xarray data array
    wt_obs = xr.DataArray(wt_rf_np_obs, coords=[seasons, years, lon, lat ], dims=['season','time','lon','lat'], name='counts')
    wt_obs.attrs['units'] = 'relative frequency per year (%)'

    #start verificaiton
    for sea in np.arange(len(seasons)):
        obs_seas = wt_obs.sel(season=seasons[sea])
        for en in np.arange(len(experiment_out)):
            #get ensemble mean of the annual relative LWT count frequencies for each experiment set in <experiment_out>, this is the yearly signal of the experiment
            mod_seas_mean = wt_mod.sel(season=seasons[sea],experiment=experiment_out[en]).mean(dim='run_index')
            #get ensemble standard deviation of the relative LWT count frequencies for each experiment set in <experiment_out>, this is the yearly noise term of the experiment
            mod_seas_std = wt_mod.sel(season=seasons[sea],experiment=experiment_out[en]).std(dim='run_index')
            
            #get correlation coefficients and pvalues
            pearson_r = xs.pearson_r(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('pearson_r')
            pearson_pval = xs.pearson_r_p_value(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('pearson_pval')
            pearson_pval_effn = xs.pearson_r_eff_p_value(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('pearson_pval_effn')        
            spearman_r = xs.spearman_r(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('spearman_r')
            spearman_pval = xs.spearman_r_p_value(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('spearman_pval')
            spearman_pval_effn = xs.spearman_r_eff_p_value(obs_seas,mod_seas_mean,dim='time',skipna=True).rename('spearman_pval_effn')
            
            # #now map the results
            # xx,yy = np.meshgrid(lon.values,lat.values)
            # sig_ind = (spearman_pval.values < 0.05) & (spearman_r.values > 0)
            #plot nh and sh separately
            spearman_r_nh = spearman_r.isel(lat=spearman_r.lat > 0)
            spearman_r_sh = spearman_r.isel(lat=spearman_r.lat < 0)
            spearman_pval_nh = spearman_pval.isel(lat=spearman_pval.lat > 0)
            spearman_pval_sh = spearman_pval.isel(lat=spearman_pval.lat < 0)
            sig_ind_nh = (spearman_pval_nh.values < 0.05) & (spearman_r_nh.values > 0)
            sig_ind_sh = (spearman_pval_sh.values < 0.05) & (spearman_r_sh.values > 0)
            xx_nh, yy_nh = np.meshgrid(spearman_r_nh.lon.values,spearman_r_nh.lat.values)
            xx_sh, yy_sh = np.meshgrid(spearman_r_sh.lon.values,spearman_r_sh.lat.values)
            
            title = 'Rank corr. coeff., '+tarwts_name[lwt]+', '+seasons[sea]+', '+experiment_out[en]+' vs '+obs.upper()+', '+str(taryears[0])+', '+str(taryears[1])
            savedir = figs+'/'+experiment[en]+'/global'
            if os.path.isdir(savedir) != True:
                os.makedirs(savedir)
            halfres = (lat.values[1]-lat.values[0])/2
            cbarlabel = 'Rank correlation coefficient'
            savename_nh = savedir+'/spearman_'+tarwts_name[lwt]+'_'+seasons[sea]+'_'+experiment_out[en]+'_vs_'+obs+'_nh_'+str(taryears[0])+'_'+str(taryears[1])+'.'+outformat
            savename_sh = savedir+'/spearman_'+tarwts_name[lwt]+'_'+seasons[sea]+'_'+experiment_out[en]+'_vs_'+obs+'_sh_'+str(taryears[0])+'_'+str(taryears[1])+'.'+outformat
            get_map_lowfreq_var(np.transpose(spearman_r_nh.values),xx_nh,yy_nh,np.transpose(sig_ind_nh),-1,1,dpival,title,savename_nh,halfres,colormap,titlesize,cbarlabel,map_proj_nh,origpoint=None)
            get_map_lowfreq_var(np.transpose(spearman_r_sh.values),xx_sh,yy_sh,np.transpose(sig_ind_sh),-1,1,dpival,title,savename_sh,halfres,colormap,titlesize,cbarlabel,map_proj_sh,origpoint=None)

            # ##plot nh and sh jointly
            # fig, ax = plt.subplots(subplot_kw={'projection': map_proj})
            # pcm1 = ax.pcolormesh(xx, yy, np.transpose(spearman_r.values), cmap=colormap, vmin=-1, vmax=1, transform=ccrs.PlateCarree(), shading='auto')       
            # ax.coastlines()
            # cbar1 = fig.colorbar(pcm1, ax=ax, label=cbarlabel)            
            # ##alternative joint plot
            # # image = ax.pcolormesh(xx, yy, np.transpose(spearman_r.values), vmin=-1, vmax=1, cmap=colormap, transform=ccrs.PlateCarree())
            # # cbar = plt.colorbar(image,orientation='vertical', shrink = 0.6)
            # # cbar.set_label(cbarlabel_f, rotation=270, labelpad=+12, y=0.5, fontsize=titlesize_f)            
            # plt.title(title, fontsize=titlesize)
            # plt.savefig(savename,dpi=dpival)
            # plt.close('all')
            
            #and clean up
            obs_seas.close()
            mod_seas_mean.close()
            mod_seas_std.close()
            pearson_r.close()
            pearson_pval.close()
            pearson_pval_effn.close()
            spearman_r.close()
            spearman_r_nh.close()
            spearman_r_sh.close()
            spearman_pval_nh.close()
            spearman_pval_sh.close()
            spearman_pval.close()
            spearman_pval_effn.close()

print('INFO: skill_maps_from_mon_counts.py has run successfully!')
